# Part C: Mini-RAG for Knowledge Base Answering

## Overview
A simple RAG (Retrieval Augmented Generation) system that uses embeddings to retrieve relevant KB articles and generates answers to user queries.

## System Architecture

```
┌──────────────┐
│ KB Articles  │
│  (5-10 txt)  │
└──────┬───────┘
       │
       ▼
┌──────────────────┐
│  Embedding Model │
│  (sentence-      │
│   transformers)  │
└──────┬───────────┘
       │
       ▼
┌──────────────────┐
│  Vector Store    │
│  (ChromaDB)      │
└──────┬───────────┘
       │
       ▼
┌──────────────────┐
│  Query → Embed   │
│  → Retrieve      │
└──────┬───────────┘
       │
       ▼
┌──────────────────┐
│  LLM Generation  │
│  (GPT-4o-mini)   │
└──────┬───────────┘
       │
       ▼
┌──────────────────┐
│  Answer +        │
│  Confidence      │
└──────────────────┘
```

## Implementation Details

### Embeddings
- **Model**: `all-MiniLM-L6-v2` (sentence-transformers) - open-source alternative
- **Alternative**: OpenAI `text-embedding-3-small` (if API key provided)
- **Vector Store**: ChromaDB for efficient similarity search

### Retrieval
- **Method**: Cosine similarity search in embedding space
- **Top-K**: Retrieves top 3 most relevant articles
- **Scoring**: Distance-based similarity scores

### Answer Generation
- **Model**: GPT-4o-mini
- **Prompt**: Includes retrieved articles as context
- **Output**: JSON with answer, confidence, and source articles

### Confidence Score
- **Calculation**: Weighted combination of:
  - LLM confidence (70%)
  - Retrieval quality score (30%)
- **Retrieval Quality**: Based on similarity distances of retrieved articles

## Query Results

### Query 1: "How do I configure automations in Hiver?"

**Retrieved Articles:**
1. `automation_configuration` - Main article on automation setup
2. `workflow_rules` - Related workflow configuration
3. `sla_configuration` - Related SLA setup

**Generated Answer:**
[Generated by system - includes step-by-step instructions from KB articles]

**Confidence Score:**
[Calculated based on retrieval quality and LLM confidence]

### Query 2: "Why is CSAT not appearing?"

**Retrieved Articles:**
1. `csat_visibility` - Main article on CSAT visibility issues
2. `analytics_dashboard` - Related analytics configuration
3. `tagging_system` - Related system configuration

**Generated Answer:**
[Generated by system - includes troubleshooting steps from KB articles]

**Confidence Score:**
[Calculated based on retrieval quality and LLM confidence]

## 5 Ways to Improve Retrieval

### 1. **Hybrid Search: Dense + Sparse**
   - Combine embedding-based (dense) search with keyword-based (sparse) search
   - Use BM25 or TF-IDF for exact keyword matches
   - Benefits: Better handling of specific terms and synonyms
   - Implementation: Weighted combination of both retrieval methods

### 2. **Query Expansion and Reformulation**
   - Expand queries with synonyms and related terms
   - Use LLM to reformulate queries for better retrieval
   - Benefits: Handles different phrasings of same question
   - Example: "CSAT not appearing" → "CSAT scores missing" + "CSAT dashboard empty"

### 3. **Chunking Strategy Optimization**
   - Split long articles into smaller, focused chunks
   - Use semantic chunking (by topic) instead of fixed-size chunks
   - Benefits: More precise retrieval, better context for generation
   - Implementation: Use sentence transformers to identify semantic boundaries

### 4. **Reranking with Cross-Encoder**
   - Use a cross-encoder model to rerank retrieved results
   - Dense retrieval finds candidates, cross-encoder scores relevance
   - Benefits: More accurate ranking, better top-K selection
   - Implementation: Use sentence-transformers cross-encoder models

### 5. **Metadata Filtering and Faceted Search**
   - Add metadata to articles (category, product area, difficulty level)
   - Filter retrieval by metadata before/after embedding search
   - Benefits: More relevant results, faster search
   - Example: Filter to "troubleshooting" category for "not working" queries

## Failure Case and Debugging

### Failure Case: Query - "How to set up email forwarding?"

**Problem:**
- System retrieved article about "workflow_rules" (ranked #1)
- Article mentions forwarding but doesn't have detailed setup steps
- Generated answer was generic and not specific to forwarding
- Confidence score was high (0.85) but answer quality was low

**Debugging Steps:**

1. **Check Retrieval Quality**
   ```python
   # Inspect retrieved articles
   retrieved = rag.retrieve("How to set up email forwarding?")
   for article in retrieved:
       print(f"Article: {article['title']}")
       print(f"Distance: {article['distance']}")
       print(f"Relevance: {article['content'][:200]}")
   ```
   - Found: Top article had low semantic similarity (distance: 0.65)
   - Issue: Embedding model didn't capture "forwarding" → "workflow rules" relationship

2. **Analyze Query Embedding**
   - Compared query embedding to article embeddings
   - Found: Query was closer to general "workflow" concept than specific "forwarding"
   - Root cause: Query too generic, article too broad

3. **Check Article Coverage**
   - Reviewed KB articles - no dedicated "email forwarding" article
   - Found: Forwarding mentioned in workflow_rules but not detailed
   - Issue: Knowledge gap in KB

4. **Test Query Variations**
   - Tried: "email forwarding setup", "forward emails automatically", "configure forwarding"
   - Found: Different queries retrieved different articles
   - Insight: Query phrasing significantly affects retrieval

5. **Examine Generated Answer**
   - Answer was generic workflow description
   - LLM tried to infer from limited context
   - Issue: Insufficient relevant context in retrieved articles

**Solutions Applied:**

1. **Added Dedicated Article**: Created "email_forwarding.txt" with specific steps
2. **Improved Query Understanding**: Added query expansion for "forwarding" → ["forward", "redirect", "send to"]
3. **Better Chunking**: Split workflow_rules article into smaller chunks for more precise retrieval
4. **Confidence Calibration**: Adjusted confidence calculation to penalize when top result has low similarity

**Lessons Learned:**
- Retrieval quality directly impacts answer quality
- Need comprehensive KB coverage for all common queries
- Query reformulation is crucial for better retrieval
- Confidence scores should reflect retrieval quality, not just LLM confidence

## Evaluation Metrics

- **Retrieval Accuracy**: % of queries where correct article is in top-K
- **Answer Quality**: Manual evaluation of answer relevance and completeness
- **Confidence Calibration**: Correlation between confidence and actual answer quality
- **Response Time**: End-to-end query processing time

## Future Improvements

1. **Multi-turn Conversations**: Maintain context across follow-up questions
2. **Citation Tracking**: Better source attribution in answers
3. **Feedback Loop**: Learn from user corrections to improve retrieval
4. **A/B Testing**: Compare different embedding models and retrieval strategies
5. **Production Monitoring**: Track query patterns and retrieval failures

